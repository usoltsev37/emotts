{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b24d8ec8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import re\n",
    "import subprocess\n",
    "import torch\n",
    "from pathlib import Path\n",
    "\n",
    "from scipy.io.wavfile import write as wav_write\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from src.models.hifi_gan.models import Generator, load_model as load_hifi\n",
    "from src.train_config import TrainParams, load_config\n",
    "from src.preprocessing.text.cleaners import english_cleaners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0211ac2f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c09b5a1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = load_config(\"configs/fastspeech2.yml\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5a340a2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = \"cuda:0\"  # config.device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c3700700",
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_path = Path(f\"checkpoints/{config.checkpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c66c559b",
   "metadata": {},
   "outputs": [],
   "source": [
    "generators = [file for file in Path(config.pretrained_hifi).rglob(\"*\") if file.name.startswith(\"g_\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe515731",
   "metadata": {},
   "outputs": [],
   "source": [
    "G2P_MODEL_PATH = \"models/en/g2p/english_g2p.zip\"\n",
    "G2P_OUTPUT_PATH = \"predictions/to_g2p.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "829bd271",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_file(user_query: str) -> None:\n",
    "    text_path = Path(\"tmp.txt\")\n",
    "    with open(text_path, \"w\") as fout:\n",
    "        normalized_content = english_cleaners(user_query)\n",
    "        normalized_content = \" \".join(re.findall(\"[a-zA-Z]+\", normalized_content))\n",
    "        fout.write(normalized_content)\n",
    "    subprocess.call(\n",
    "        [\"mfa\", \"g2p\", G2P_MODEL_PATH, text_path.absolute(), G2P_OUTPUT_PATH]\n",
    "    )\n",
    "    text_path.unlink()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "01a2685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "default = {\"he\": \"HH IY1\", \"she\": \"SH IY1\", \"we\": \"W IY1\", \"be\": \"B IY0\", \"the\": \"DH AH0\", \"whenever\": \"W EH0 N EH1 V ER0\", \"year\": \"AH0 Y IH1 R\"}\n",
    "\n",
    "def parse_g2p(PHONEMES_TO_IDS, g2p_path: str = G2P_OUTPUT_PATH):\n",
    "    with open(g2p_path, \"r\") as fin:\n",
    "        phonemes_ids = []\n",
    "        phonemes = []\n",
    "        phonemes_ids.append(PHONEMES_TO_IDS[\"\"])\n",
    "        for line in fin:\n",
    "            word, word_to_phones = line.rstrip().split(\"\\t\", 1)\n",
    "            if word in default:\n",
    "                word_to_phones = default[word]\n",
    "            phonemes.extend(word_to_phones.split(\" \"))\n",
    "            phonemes_ids.extend(\n",
    "                [PHONEMES_TO_IDS[ph] for ph in word_to_phones.split(\" \")]\n",
    "            )\n",
    "        phonemes_ids.append(PHONEMES_TO_IDS[\"\"])\n",
    "    return phonemes_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a5f7a3f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = [\n",
    "    'Do you realize what time it is?',\n",
    "    'He comes back to the valley.',\n",
    "    'This dress does not look worth much!',\n",
    "    'What happened tonight has nothing to do with Henry.',\n",
    "    'Today, five years later, we are facing a similar situation.',\n",
    "    'When I saw you kissing, you looked really happy.',\n",
    "    'Only one vehicle may be allowed to park at any given time.',\n",
    "    'The deadlines are indeed very tight.',\n",
    "    \"I'm glad you enjoyed yourself.\",\n",
    "    'What are you still doing here?',\n",
    "    'This is an animal that is admired for its whiteness and cleanliness.  ',\n",
    "    'Perhaps there is another way to pose these issues.',\n",
    "    \"Your students' test scores drop lower and lower every year.\",\n",
    "    'Wherever her tears fell, a fruit tree grew.',\n",
    "    'I was about to head back to my hotel and go to sleep.',\n",
    "    'You said she really helped last time.',\n",
    "    'My favorite season, spring, is here.',\n",
    "    \"He's the rich guy who built the airplanes.\",\n",
    "    'Otto and Elizabeth gave it to us, for the wedding - incredibly generous.',\n",
    "    'Look, the police said that there was nothing stolen from the house.',\n",
    "    'And I suppose we can thank your brother for that.',\n",
    "    \"That's a pretty dangerous thing you're doing.\",\n",
    "    'He arrived in Japan for the first time at the age of twenty six.',\n",
    "    'Sam thought we were having fun being together.',\n",
    "    \"Well, the true value of something isn't always determined by its price.\",\n",
    "    \"No, it's not polite to discuss a lady's age.\",\n",
    "    \"Just another quarter-mile and I don't have to be tolerant ever again.\",\n",
    "    \"But Jones' apartment had only been rented out for a week.\",\n",
    "    'What your perfect day would have been like?',\n",
    "    'Not a very useful skill, especially when the money runs out.',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1ba005d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "huawei_phon_to_mfa_phon_ = {\n",
    "    'AX1': 'AO1',\n",
    "    'UX1': 'UW1'\n",
    "}\n",
    "\n",
    "huawei_phones = [\n",
    "    ' D UW1 Y UW1 R IY1 AH0 L AY2 Z W AH1 T T AY1 M IH1 T IH1 Z  ',\n",
    "    ' HH IY1 K AH1 M Z B AE1 K T UW1 DH AH0 V AE1 L IY0  ',\n",
    "    ' DH IH1 S D R EH1 S D AH1 Z N AA1 T L UH1 K W ER1 TH M AH1 CH  ',\n",
    "    ' W AH1 T HH AE1 P AH0 N D T AH0 N AY1 T HH AE1 Z N AH1 TH IH0 NG T UW1 D UW1 W IH1 DH HH EH1 N R IY0  ',\n",
    "    ' T AH0 D EY1  F AY1 V Y IH1 R Z L EY1 T ER0  W IY1 AA1 R F EY1 S IH0 NG AH0 S IH1 M AH0 L ER0 S IH2 CH UW0 EY1 SH AH0 N  ',\n",
    "    ' W EH1 N AY1 S AO1 Y UW1 K IH1 S IH0 NG  Y UW1 L UH1 K T R IH1 L IY0 HH AE1 P IY0  ',\n",
    "    ' OW1 N L IY0 W AH1 N V IY1 HH IH0 K AH0 L M EY1 B IY1 AH0 L AW1 D T UW1 P AA1 R K AE1 T EH1 N IY0 G IH1 V AH0 N T AY1 M  ',\n",
    "    ' DH AH0 D EH1 D L AY2 N Z AA1 R IH2 N D IY1 D V EH1 R IY0 T AY1 T  ',\n",
    "    ' AY1 EH1 M G L AE1 D Y UW1 EH2 N JH OY1 D Y ER0 S EH1 L F  ',\n",
    "    ' W AH1 T AA1 R Y UW1 S T IH1 L D UW1 IH0 NG HH IY1 R  ',\n",
    "    ' DH IH1 S IH1 Z AE1 N AE1 N AH0 M AH0 L DH AE1 T IH1 Z AH0 D M AY1 ER0 D F AO1 R IH1 T S W AY1 T N AH0 S AH0 N D K L EH1 N L IY0 N IH0 S  ',\n",
    "    ' P ER0 HH AE1 P S DH EH1 R IH1 Z AH0 N AH1 DH ER0 W EY1 T UW1 P OW1 Z DH IY1 Z IH1 SH UW0 Z  ',\n",
    "    ' Y AO1 R S T UW1 D AH0 N T S T EH1 S T S K AO1 R Z D R AA1 P L OW1 ER0 AH0 N D L OW1 ER0 EH1 V ER0 IY0 Y IH1 R  ',\n",
    "    ' W EH0 R EH1 V ER0 HH ER1 T IH1 R Z F EH1 L  AH0 F R UW1 T T R IY1 G R UW1  ',\n",
    "    ' AY1 W AA1 Z AH0 B AW1 T T UW1 HH EH1 D B AE1 K T UW1 M AY1 HH OW0 T EH1 L AH0 N D G OW1 T UW1 S L IY1 P  ',\n",
    "    ' Y UW1 S EH1 D SH IY1 R IH1 L IY0 HH EH1 L P T L AE1 S T T AY1 M  ',\n",
    "    ' M AY1 F EY1 V ER0 IH0 T S IY1 Z AH0 N  S P R IH1 NG  IH1 Z HH IY1 R  ',\n",
    "    ' HH IY1 EH1 S DH AH0 R IH1 CH G AY1 HH UW1 B IH1 L T DH IY0 EH1 R P L EY0 N Z  ',\n",
    "    ' AA1 T OW2 AH0 N D IH0 L IH1 Z AH0 B AH0 TH G EY1 V IH1 T T UW1 AH1 S  F AO1 R DH AH0 W EH1 D IH0 NG  IH2 N K R EH1 D AH0 B L IY0 JH EH1 N ER0 AH0 S  ',\n",
    "    ' L UH1 K  DH AH0 P AH0 L IY1 S S EH1 D DH AE1 T DH EH1 R W AA1 Z N AH1 TH IH0 NG S T OW1 L AH0 N F R AH1 M DH AH0 HH AW1 S  ',\n",
    "    ' AH0 N D AY1 S AH0 P OW1 Z W IY1 K AE1 N TH AE1 NG K Y AO1 R B R AH1 DH ER0 F AO1 R DH AE1 T  ',\n",
    "    ' DH AE1 T EH1 S EY0 P R IH1 T IY0 D EY1 N JH ER0 AH0 S TH IH1 NG Y UW1 R EY1 D UW1 IH0 NG  ',\n",
    "    ' HH IY1 ER0 AY1 V D IH0 N JH AH0 P AE1 N F AO1 R DH AH0 F ER1 S T T AY1 M AE1 T DH IY0 EY1 JH AH1 V T W EH1 N T IY0 S IH1 K S  ',\n",
    "    ' S AE1 M TH AO1 T W IY1 W ER1 HH AE1 V IH0 NG F AH1 N B IY1 IH0 NG T AH0 G EH1 DH ER0  ',\n",
    "    ' W EH1 L  DH AH0 T R UW1 V AE1 L Y UW0 AH1 V S AH1 M TH IH0 NG IH1 S N T IY1 AO1 L W EY2 Z D IH0 T ER1 M AH0 N D B AY1 IH1 T S P R AY1 S  ',\n",
    "    ' N OW1  IH1 T EH1 S N AA1 T P AH0 L AY1 T T UW1 D IH0 S K AH1 S AH0 L EY1 D IY0 EH1 S EY1 JH  ',\n",
    "    ' JH AH1 S T AH0 N AH1 DH ER0 K W AO1 R T ER0 M AY1 L AH0 N D AY1 D AA1 N T IY1 HH AE1 V T UW1 B IY1 T AA1 L ER0 AH0 N T EH1 V ER0 AH0 G EH1 N  ',\n",
    "    ' B AH1 T JH OW1 N Z AH0 P AA1 R T M AH0 N T HH AE1 D OW1 N L IY0 B IH1 N R EH1 N T IH0 D AW1 T F AO1 R AH0 W IY1 K  ',\n",
    "    ' W AH1 T Y AO1 R P ER1 F IH1 K T D EY1 W UH1 D HH AE1 V B IH1 N L AY1 K  ',\n",
    "    ' N AA1 T AH0 V EH1 R IY0 Y UW1 S F AH0 L S K IH1 L  AH0 S P EH1 SH L IY0 W EH1 N DH AH0 M AH1 N IY0 R AH1 N Z AW1 T  ',\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "871aa1eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_phones(PHONEMES_TO_IDS, phones):\n",
    "    \"\"\"For new ones\"\"\"\n",
    "    phonemes_ids = (\n",
    "       [PHONEMES_TO_IDS[ph] for ph in phones.rstrip().split()]\n",
    "    )\n",
    "    return phonemes_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "521223bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "phonemes_list = []\n",
    "with open(checkpoint_path / \"fastspeech2\"/ \"phonemes.json\") as f:\n",
    "    phonemes_to_ids = json.load(f)\n",
    "for hp in huawei_phones:\n",
    "    phoneme_ids = to_phones(phonemes_to_ids, hp)\n",
    "    phonemes_list.append(phoneme_ids)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc14f2e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#phonemes_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f864d6f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastspeech2_model = torch.load(checkpoint_path / \"fastspeech2\" / \"fastspeech2_model.pth\", map_location=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "540d9c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "fastspeech2_model = fastspeech2_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d9abd41f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tacotron_batch(\n",
    "    phonemes_ids, speaker_id, device\n",
    "):\n",
    "    text_lengths_tensor = torch.LongTensor([len(phonemes_ids)]).to(device)\n",
    "    phonemes_ids_tensor = torch.LongTensor(phonemes_ids).unsqueeze(0).to(device)\n",
    "    speaker_ids_tensor = torch.LongTensor([speaker_id]).to(device)\n",
    "    return phonemes_ids_tensor, text_lengths_tensor, speaker_ids_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fedb40a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_pathes = Path(\"references/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6c4506b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_path = Path(f\"generated_hifi/{config.checkpoint_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3698f854",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(checkpoint_path / \"fastspeech2\"/ \"speakers.json\") as f:\n",
    "    speaker_to_id = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "998b74bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "mels_mean = torch.load(checkpoint_path / \"fastspeech2\" / \"mels_mean.pth\", map_location=device).float()\n",
    "mels_std = torch.load(checkpoint_path / \"fastspeech2\" / \"mels_std.pth\", map_location=device).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e175d561",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "371521960eac494288b5622e1300aa66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/95 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [26]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m batch \u001b[38;5;241m=\u001b[39m get_tacotron_batch(phonemes, speaker_id, device)\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 8\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfastspeech2_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      9\u001b[0m     mels \u001b[38;5;241m=\u001b[39m output[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mpermute(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     10\u001b[0m     mels \u001b[38;5;241m=\u001b[39m mels \u001b[38;5;241m*\u001b[39m mels_std\u001b[38;5;241m.\u001b[39mto(device) \u001b[38;5;241m+\u001b[39m mels_mean\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/graduaded_project/emotts/src/models/fastspeech2/fastspeech2.py:139\u001b[0m, in \u001b[0;36mFastSpeech2.inference\u001b[0;34m(self, batch, p_control, e_control, d_control)\u001b[0m\n\u001b[1;32m    114\u001b[0m     output \u001b[38;5;241m=\u001b[39m output \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mspeaker_emb(speaker_ids)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\n\u001b[1;32m    115\u001b[0m         \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, max_phonemes_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    118\u001b[0m (\n\u001b[1;32m    119\u001b[0m     output,\n\u001b[1;32m    120\u001b[0m     _,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    136\u001b[0m     d_control,\n\u001b[1;32m    137\u001b[0m )\n\u001b[0;32m--> 139\u001b[0m output, mel_masks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmel_masks\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    140\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmel_linear(output)\n\u001b[1;32m    142\u001b[0m postnet_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpostnet(output) \u001b[38;5;241m+\u001b[39m output\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.9/site-packages/torch/nn/modules/module.py:1110\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1106\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1107\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1108\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1109\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1110\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1111\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1112\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/graduaded_project/emotts/src/models/fastspeech2/transformer/Models.py:151\u001b[0m, in \u001b[0;36mDecoder.forward\u001b[0;34m(self, enc_seq, mask, return_attns)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;129;01mand\u001b[39;00m enc_seq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len:\n\u001b[1;32m    149\u001b[0m     \u001b[38;5;66;03m# -- Prepare masks\u001b[39;00m\n\u001b[1;32m    150\u001b[0m     slf_attn_mask \u001b[38;5;241m=\u001b[39m mask\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, max_len, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m--> 151\u001b[0m     dec_output \u001b[38;5;241m=\u001b[39m enc_seq \u001b[38;5;241m+\u001b[39m \u001b[43mget_sinusoid_encoding_table\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m        \u001b[49m\u001b[43menc_seq\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43md_model\u001b[49m\n\u001b[1;32m    153\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m[: enc_seq\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], :]\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mexpand(batch_size, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(\n\u001b[1;32m    154\u001b[0m         enc_seq\u001b[38;5;241m.\u001b[39mdevice\n\u001b[1;32m    155\u001b[0m     )\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    157\u001b[0m     max_len \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(max_len, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmax_seq_len)\n",
      "File \u001b[0;32m~/graduaded_project/emotts/src/models/fastspeech2/transformer/Models.py:23\u001b[0m, in \u001b[0;36mget_sinusoid_encoding_table\u001b[0;34m(n_position, d_hid, padding_idx)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_posi_angle_vec\u001b[39m(position):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [cal_angle(position, hid_j) \u001b[38;5;28;01mfor\u001b[39;00m hid_j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_hid)]\n\u001b[1;32m     22\u001b[0m sinusoid_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m---> 23\u001b[0m     [get_posi_angle_vec(pos_i) \u001b[38;5;28;01mfor\u001b[39;00m pos_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_position)]\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m sinusoid_table[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(sinusoid_table[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# dim 2i\u001b[39;00m\n\u001b[1;32m     27\u001b[0m sinusoid_table[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(sinusoid_table[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# dim 2i+1\u001b[39;00m\n",
      "File \u001b[0;32m~/graduaded_project/emotts/src/models/fastspeech2/transformer/Models.py:23\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_posi_angle_vec\u001b[39m(position):\n\u001b[1;32m     20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [cal_angle(position, hid_j) \u001b[38;5;28;01mfor\u001b[39;00m hid_j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_hid)]\n\u001b[1;32m     22\u001b[0m sinusoid_table \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39marray(\n\u001b[0;32m---> 23\u001b[0m     [\u001b[43mget_posi_angle_vec\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpos_i\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m pos_i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_position)]\n\u001b[1;32m     24\u001b[0m )\n\u001b[1;32m     26\u001b[0m sinusoid_table[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39msin(sinusoid_table[:, \u001b[38;5;241m0\u001b[39m::\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# dim 2i\u001b[39;00m\n\u001b[1;32m     27\u001b[0m sinusoid_table[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcos(sinusoid_table[:, \u001b[38;5;241m1\u001b[39m::\u001b[38;5;241m2\u001b[39m])  \u001b[38;5;66;03m# dim 2i+1\u001b[39;00m\n",
      "File \u001b[0;32m~/graduaded_project/emotts/src/models/fastspeech2/transformer/Models.py:20\u001b[0m, in \u001b[0;36mget_sinusoid_encoding_table.<locals>.get_posi_angle_vec\u001b[0;34m(position)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_posi_angle_vec\u001b[39m(position):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [cal_angle(position, hid_j) \u001b[38;5;28;01mfor\u001b[39;00m hid_j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_hid)]\n",
      "File \u001b[0;32m~/graduaded_project/emotts/src/models/fastspeech2/transformer/Models.py:20\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mget_posi_angle_vec\u001b[39m(position):\n\u001b[0;32m---> 20\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m [\u001b[43mcal_angle\u001b[49m\u001b[43m(\u001b[49m\u001b[43mposition\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhid_j\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m hid_j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(d_hid)]\n",
      "File \u001b[0;32m~/graduaded_project/emotts/src/models/fastspeech2/transformer/Models.py:17\u001b[0m, in \u001b[0;36mget_sinusoid_encoding_table.<locals>.cal_angle\u001b[0;34m(position, hid_idx)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcal_angle\u001b[39m(position, hid_idx):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m position \u001b[38;5;241m/\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpower\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m10000\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mhid_idx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m/\u001b[39;49m\u001b[43m \u001b[49m\u001b[43md_hid\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "for reference in tqdm(list(reference_pathes.rglob(\"*.pkl\"))):\n",
    "    speaker = reference.parent.name\n",
    "    speaker_id = speaker_to_id[speaker]\n",
    "    ref_mel = torch.load(reference, map_location=device)\n",
    "    for i, phonemes in enumerate(phonemes_list):\n",
    "        batch = get_tacotron_batch(phonemes, speaker_id, device)\n",
    "        with torch.no_grad():\n",
    "            output = fastspeech2_model.inference(batch)\n",
    "            mels = output[1].permute(0, 2, 1).squeeze(0)\n",
    "            mels = mels * mels_std.to(device) + mels_mean.to(device)\n",
    "            x = mels.unsqueeze(0)\n",
    "            for generator_path in generators:\n",
    "                state_dict = torch.load(generator_path, map_location=\"cpu\")\n",
    "                state_dict[\"generator\"] = {k: v.to(device) for k, v in state_dict[\"generator\"].items()}\n",
    "                generator = Generator(config=config.train_hifi.model_param, num_mels=config.n_mels).to(device)\n",
    "                generator.load_state_dict(state_dict[\"generator\"])\n",
    "                generator.remove_weight_norm()\n",
    "                generator.eval()\n",
    "                y_g_hat = generator(x)\n",
    "                audio = y_g_hat.squeeze()\n",
    "                audio = audio * 32768\n",
    "                audio = audio.type(torch.int16).detach().cpu().numpy()\n",
    "                save_path = generated_path / generator_path.stem / speaker / reference.stem\n",
    "                save_path.mkdir(exist_ok=True, parents=True)\n",
    "                wav_write(save_path / f\"{i + 1}.wav\", 22050, audio)\n",
    "                torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "7e79b89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(mels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e1341b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 790, 80])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mels[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ca985c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13c72b62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fafdc76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47ef3241",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7930f7f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36a3adb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6bccefd5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c378452e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36047610",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
