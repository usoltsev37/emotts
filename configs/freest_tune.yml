sample_rate: 22050
hop_size: 256
f_min: 0
f_max: 8000
win_size: 1024
n_fft: 1024
n_mels: 80
seed: 42
batch_size: 16
grad_clip_thresh: 1.0
log_steps: 1000
iters_per_checkpoint: 50000
total_iterations: 300001
finetune: true
test_size: 0.05
device: cuda:1
lang: chinese
checkpoint_name: freest_tune
data:
  text_dir: data/freest/mfa_outputs
  mels_dir: data/freest/mels
  wav_dir: data/freest/resampled
  feature_dir: data/freest/feature_output
  mels_fastspeech2_dir: data/processed/esd/english/fastspeech2/mels
  duration_dir: data/processed/esd/english/fastspeech2/duration
  pitch_dir: data/processed/esd/english/fastspeech2/pitch
  energy_dir: data/processed/esd/english/fastspeech2/energy
  phones_dir: data/processed/esd/english/fastspeech2/phones
  finetune_speakers:
    - "0001"
    - "0002"
    - "0003"
    - "0004"
    - "0005"
    - "0006"
    - "0007"
    - "0008"
    - "0009"
    - "0010"
  ignore_speakers: [

  ]
pretrained_hifi: models/en/hifi
base_model: models/en/tacotron
fastspeech2:
  use_gst: false
  encoder_params:
    conv_filter_size: 1024
    conv_kernel_size:
      - 9
      - 1
    encoder_layer: 6
    encoder_head: 2
    encoder_hidden: 256
    encoder_dropout: 0.2
  decoder_params:
    conv_filter_size: 1024
    conv_kernel_size:
      - 9
      - 1
    decoder_layer: 6
    decoder_head: 2
    decoder_hidden: 256
    decoder_dropout: 0.2
  variance_adapter_params:
    predictor_params:
      filter_size: 256
      kernel_size: 3
      dropout: 0.5
    embedding_params:
      n_bins: 256
      pitch_quantization: linear
      energy_quantization: linear
  max_seq_len: 1000
gst_config:
  ref_enc_filters:
    - 32
    - 32
    - 64
    - 64
    - 128
    - 128
  emb_dim: 256
  num_heads: 8
  token_num: 10

train_hifi:
  split_data: true
  training_epochs: 1000
  logging_interval: 5
  checkpoint_interval: 15000
  summary_interval: 1000
  fine_tuning: true
  fmax_loss: null
  batch_size: 16
  learning_rate: 0.0002
  adam_b1: 0.8
  adam_b2: 0.99
  lr_decay: 0.999
  model_param:
    resblock: "1"
    upsample_rates:
      - 8
      - 8
      - 2
      - 2
    upsample_kernel_sizes:
      - 16
      - 16
      - 4
      - 4
    upsample_initial_channel: 512
    resblock_kernel_sizes:
      - 3
      - 7
      - 11
    resblock_dilation_sizes:
      - [1, 3, 5]
      - [1, 3, 5]
      - [1, 3, 5]
    resblock_initial_channel: 256
  segment_size: 8192
loss:
  is_reversal: false
  mels_weight: 1.0
  duration_weight: 2.0
  adversarial_weight: 0.1
optimizer:
  learning_rate: 0.001
  adam_beta1: 0.9
  adam_beta2: 0.999
  adam_epsilon: 1e-06
  reg_weight: 1e-06
scheduler:
  start_decay: 4000
  decay_steps: 50000
  decay_rate: 0.5
  last_epoch: 400000
model:
  n_frames_per_step: 1
  encoder_config:
    n_convolutions: 3
    kernel_size: 5
    conv_channel: 512
    lstm_layers: 1
    lstm_hidden: 256
    dropout: 0.1
  attention_config:
    duration_config:
      lstm_layers: 2
      lstm_hidden: 256
      dropout: 0.5
    range_config:
      lstm_layers: 2
      lstm_hidden: 256
      dropout: 0.5
    eps: 1e-6
    positional_dim: 32
    teacher_forcing_ratio: 1.0
    attention_dropout: 0.1
    positional_dropout: 0.0
  decoder_config:
    prenet_layers:
      - 256
      - 256
    prenet_dropout: 0.5
    decoder_rnn_dim: 512
    decoder_num_layers: 3
    teacher_forcing_ratio: 1.0
    dropout: 0.1
  postnet_config:
    embedding_dim: 512
    n_convolutions: 5
    kernel_size: 5
    dropout: 0.1
  mask_padding: true
  phonem_embedding_dim: 512
  speaker_embedding_dim: 256

